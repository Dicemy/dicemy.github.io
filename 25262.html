<!DOCTYPE html>
<html  lang="zh-CN" >
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color" content="#fff" id="theme-color">
  <meta name="description" content="月光照着月光,月光普照">
  <link rel="icon" href="/img/biaoti.png">
  <title>深度学习初探</title>
  
  
  <meta property="og:title" content="深度学习初探">
  
  
  <meta property="og:url" content="https://dicemy.github.io/25262.html">
  
  
  <meta property="og:img" content="/img/touxiang.jpg">
  
  
  <meta property="og:img" content="月光照着月光,月光普照">
  
  
  <meta property="og:type" content="article">
  <meta property="og:article:published_time" content="2021-08-31">
  <meta property="og:article:modified_time" content="2021-09-08">
  <meta property="og:article:author" content="Dicemy">
  
  
  
  
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
    var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
      }
    };
    setDarkmode();
  </script>
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
  </script>
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  
  <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
  <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
  <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
  
  <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">

  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">

  
  
<link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">

  
  
  
  
  
  
  <link href="/js/lib/prism/prism.min.css" rel="stylesheet" data-prism="prism">
  
  
  
<link rel="stylesheet" href="/js/lib/prism/prism-line-numbers.min.css">

  
  
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="/css/prism.css" type="text/css"></head>


<body>
  <div class="wrapper">
    
    <nav class="navbar">
  <div class="navbar-logo">
    <span class="navbar-logo-main">
      
      <img class="navbar-logo-img" src="/img/biaoti.png" alt="logo">
      
      <span class="navbar-logo-dsc">Dicemy 的博客</span>
    </span>
  </div>
  <div class="navbar-menu">
    
    <a href="/" class="navbar-menu-item">
    
    首页
    
    </a>
    
    <a href="/archives" class="navbar-menu-item">
    
    归档
    
    </a>
    
    <a href="/tags" class="navbar-menu-item">
    
    标签
    
    </a>
    
    <a href="/categories" class="navbar-menu-item">
    
    分类
    
    </a>
    
    <a href="/about" class="navbar-menu-item">
    
    关于
    
    </a>
    
    <a href="/links" class="navbar-menu-item">
    
    友链
    
    </a>
    
    <a class="navbar-menu-item darknavbar" id="dark"><i class="iconfont icon-weather"></i></a>
    <a class="navbar-menu-item searchnavbar" id="search"><i class="iconfont icon-search" style="font-size: 1.2rem; font-weight: 400;"></i></a>
  </div>
</nav>
    
    <div id="local-search" style="display: none;">
      <input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容...">
      <div id="search-content"></div>
    </div>
    
    <div class="section-wrap">
      <div class="container">
        <div class="columns">
          <main class="main-column">
<article class="card card-content">
  <header>
    <h1 class="post-title">
      深度学习初探
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2021-08-31T11:27:39.000Z">
      <i class="iconfont icon-calendar" style="margin-right: 2px;"></i>
      <span>2021-08-31</span>
    </time>
    
    <span class="dot"></span>
    
    <a href="/categories/技术/" class="post-meta-link">技术</a>
    
    
    
    <span class="dot"></span>
    <span>2k 字</span>
    
  </div>
  
  </header>
  <div id="section" class="post-content">
    <h2 id="二分分类"><a href="#二分分类" class="headerlink" title="二分分类"></a>二分分类</h2><h3 id="什么是Logistic-回归？"><a href="#什么是Logistic-回归？" class="headerlink" title="什么是Logistic 回归？"></a>什么是Logistic 回归？</h3><p>logistic 回归函数：来对一组输入做一个 0 ~ 1 的概率判断。</p>
<p>x为输入的特征向量，得到的回归函数值为 <strong>sigma(w^(T) * x + b)</strong> ，参数为w和b，是一个在0和1中间的一个概率。<strong>sigma(z)</strong> 是  <strong>1 / 1 + e ^ (- z)</strong> 。</p>
<p><strong>损失函数</strong>：lost(y^, y) = <strong>- ( y log y^ + (1 - y) log(1 - y^) )</strong> （其中y^为y的预测值）。越小越好。</p>
<p><strong>成本函数</strong>：J(w, b) = <strong>1 / m * Σ lost(y^, y) = -1 / m * Σ(y log y^ + (1 - y)log(1 - y^))</strong> 。为m组损失函数的平均值，越小越好。</p>
<p><strong>梯度下降法</strong>：<strong>w := w - αd(J(w, b)) / d(w) ，b := b - αd(J(w, b)) / d(b)</strong> 。α是学习率。</p>
<p>梯度就是函数对变量的偏导。</p>
<p>正向传播：用参数计算出函数的值</p>
<p>反向传播：利用函数来计算每一个参数的导数</p>
<p>对Logistic 回归的一组数据进行梯度下降法：</p>
<img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210901232913.png" style="zoom:150%;" / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210901232913.png" class="lozad post-image">

<h3 id="Logistic-回归的Python实现"><a href="#Logistic-回归的Python实现" class="headerlink" title="Logistic 回归的Python实现"></a>Logistic 回归的Python实现</h3><p>对Logistic 回归的m组数据进行一次梯度下降法：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">J &#x3D; 0; dw1 &#x3D; 0; dw2 &#x3D; 0; db &#x3D; 0;
for i &#x3D; 1 to m
    z(i) &#x3D; wx(i)+b;
    a(i) &#x3D; sigmoid(z(i));
    J +&#x3D; -[y(i)log(a(i))+(1-y(i)）log(1-a(i));
    dz(i) &#x3D; a(i)-y(i);
    dw1 +&#x3D; x1(i)dz(i);
    dw2 +&#x3D; x2(i)dz(i);
    db +&#x3D; dz(i);
J &#x2F;&#x3D; m;
dw1 &#x2F;&#x3D; m;
dw2 &#x2F;&#x3D; m;
db &#x2F;&#x3D; m;
w &#x3D; w - alpha * dw
b &#x3D; b - alpha * db<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="如何减少循环的次数？"><a href="#如何减少循环的次数？" class="headerlink" title="如何减少循环的次数？"></a>如何减少循环的次数？</h3><p>使用<strong>向量化</strong>来减少显式For循环。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as np #导入numpy库
a &#x3D; np.array([1,2,3,4]) #创建一个数据a
print(a)
import time #导入时间库
a &#x3D; np.random.rand(1000000)
b &#x3D; np.random.rand(1000000) #通过round随机得到两个一百万维度的数组
tic &#x3D; time.time() #现在测量一下当前时间
#向量化的版本
c &#x3D; np.dot(a,b) #对a，b进行矩阵乘法
toc &#x3D; time.time()
print(“Vectorized version:” + str(1000*(toc-tic)) +”ms”) #打印一下向量化的版本的时间
#继续增加非向量化的版本
c &#x3D; 0
tic &#x3D; time.time()
for i in range(1000000):
    c +&#x3D; a[i]*b[i]
toc &#x3D; time.time()
print(c)
print(“For loop:” + str(1000*(toc-tic)) + “ms”)#打印for循环的版本的时间<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>通过使用<strong>numpy</strong>中的函数来进行向量运算从而减少显式For循环。</p>
<p>可以使用向量化完全消除显式的For循环：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">Z &#x3D; np.dot(w.T, x)+b
A &#x3D; σ(Z)
dZ &#x3D; A − Y
dw &#x3D; 1 &#x2F; m ∗ X ∗ dZ.T
db &#x3D; 1 &#x2F; m ∗ np.sum(dZ)
w &#x3D; w − α ∗ dw
b &#x3D; b − α ∗ db<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用for循环，所以我们就通过一次迭代实现一次梯度下降，但如果你希望多次迭代进行梯度下降，那么仍然需要for循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比较舒服一些。</p>
</blockquote>
<p>学习进度：吴恩达深度学习课程第一课前两周课（24节）</p>
<h2 id="浅层神经网络"><a href="#浅层神经网络" class="headerlink" title="浅层神经网络"></a>浅层神经网络</h2><h3 id="什么是双层神经网络？"><a href="#什么是双层神经网络？" class="headerlink" title="什么是双层神经网络？"></a>什么是双层神经网络？</h3><p>神经网络的大致组成：<strong>输入层，隐藏层，输出层。</strong></p>
<p>logistic 回归只有输入层和输出层。</p>
<p>双层神经网络的两层是指隐藏层和输出层，输入层不算。（也可以叫单隐层神经网络）</p>
<img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210902224803.png" alt="img"  / srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210902224803.png" class="lozad post-image">

<p>上图是一个双层神经网络的示意图。</p>
<p>对比一般的Logistic 回归，对于一组输入来说，双层神经网络有多个a在第一层的输出到第二层去。在输出层中，输入是隐藏层中运算得到的a值，输出是经过Logistic 回归得到的最终值。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210902224933.png" alt="在这里插入图片描述" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210902224933.png" class="lozad post-image"></p>
<p>上图是隐藏层中一个神经元的具体构造。</p>
<p>同时我们也希望可以将这个双层神经网络进行向量化从而来简化我们的运算。</p>
<h3 id="如何将双层神经网络进行向量化？"><a href="#如何将双层神经网络进行向量化？" class="headerlink" title="如何将双层神经网络进行向量化？"></a>如何将双层神经网络进行向量化？</h3><p><img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908094249.png" alt="img" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908094249.png" class="lozad post-image"></p>
<p>该图将一组数据进行了向量化，通过上图的运算，可以更快地得出一组数据经过一次双层神经网络处理的结果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908100221.png" alt="img" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908100221.png" class="lozad post-image"></p>
<p>如图的操作可以将多组数据进行向量化运算，即，经过一次操作对多组输入进行一次双层神行网络处理。</p>
<h3 id="如何选择激活函数？"><a href="#如何选择激活函数？" class="headerlink" title="如何选择激活函数？"></a>如何选择激活函数？</h3><p>常见的激活分类有：<strong>σ函数，tanh函数，ReUL函数，Leaky Relu函数</strong>。</p>
<p>对于tanh函数，是由σ函数向下平移和伸缩后得到的。相比于σ函数，数据的平均值更接近0，这会使下一层学习简单一点。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908101454.png" alt="img" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908101454.png" class="lozad post-image"></p>
<p>选用适合的激活函数可以加快机器学习的速度，常用的<strong>Relu</strong>函数的优点如下：</p>
<blockquote>
<p>第一，在 z 的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。</p>
<p>第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)</p>
<p> z 在ReLu的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
</blockquote>
<p>概括一下不同激活函数的过程和结论：</p>
<blockquote>
<p><strong>sigmoid激活函数</strong>：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh激活函数</strong>：tanh是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu激活函数</strong>：最常用的默认函数，，如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu。公式 a = max( 0.01 z , z ) <em>为什么常数是0.01？</em>当然，可以为学习算法选择不同的参数。（注意ReLu函数是非线性的）</p>
</blockquote>
<p><strong>注意：</strong>在选择激活函数的时候，要选择非线性函数，因为多层线性函数的组合得到的结果还是线性的，那么神经网络只是把输入线性组合再输出。只有在做机器学习中的回归问题的时候才可能会选择线性函数作为激活函数。</p>
<h3 id="激活函数的导数"><a href="#激活函数的导数" class="headerlink" title="激活函数的导数"></a>激活函数的导数</h3><p>对于常见的几种激活函数的导数：</p>
<blockquote>
<p><strong>sigmoid函数 **σ(x) = a 的导数为 **a (1 - a)</strong></p>
<p><strong>tanh函数</strong> tanh(x) = a 的导数为 <strong>1 - a^2</strong></p>
<p><strong>Relu函数</strong> max(0, x) = a 的导数在 x &lt; 0 的时候为0， 在 x &gt; 0 的时候为1，在 x = 0 的时候没有定义，一般手动定义为 0 或者 1 。</p>
<p><strong>Leaky Relu函数</strong> max(0,01x, x) = a 的导数在 x &lt; 0 的时候为 0,01 ， 在 x &gt; 0 的时候为1，在 x = 0 的时候没有定义，一般手动定义为 0 或者 1 。</p>
</blockquote>
<h3 id="双层神经网络的梯度下降法（反向传播的实现）"><a href="#双层神经网络的梯度下降法（反向传播的实现）" class="headerlink" title="双层神经网络的梯度下降法（反向传播的实现）"></a>双层神经网络的梯度下降法（反向传播的实现）</h3><p>由以上学习可知双层神经网络的正向传播就是按照所给的基本公式进行计算，而反向传播可以在每一轮中对模型中的参数进行调整。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908112412.png" alt="image-20210908112409534" srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="https://cdn.jsdelivr.net/gh/Dicemy/blog-img/20210908112412.png" class="lozad post-image"></p>
<p>反向传播与普通<strong>Logistic</strong>回归不同的地方在于计算第一层的dz时的差别。</p>
<blockquote>
<p>这里np.sum是python的numpy命令，axis=1表示水平相加求和，keepdims是防止python输出那些古怪的秩数 ( n , )，加上这个确保矩阵 db[ 2 ] 这个向量输出的维度为 ( n , 1 ) 这样标准的形式。</p>
</blockquote>

  </div>
  <div>
  
  </div>
</article>
<div class="nav">
  
  
  <div class="nav-item-next">
    <a href="/43344.html" class="nav-link">
      <div>
        <div class="nav-label">下一篇</div>
        
        <div class="nav-title">河南大学暑假集训的日常（4） </div>
        
      </div>
      <i class="iconfont icon-right nav-next-icon"></i>
    </a>
  </div>
  
</div>

<div class="card card-content comment-card" style="margin-top: 16px;">
  <div class="comment-card-title">评论</div>
  
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: 'FE3KQNXvpvsHJmJl9idhXhp1-9Nh9j0Va',
        appKey: 'CLLAhwQhbW5KCgp3CCPkAItO',
        placeholder: 'Just go go',
        path: window.location.pathname,
        avatar: 'mp',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'false',
        highlight: true,
        recordIP: false,
        
        
        
        enableQQ: 'true',
        requiredFields: [],
      });
    };
  </script>

</div>

<div class="card card-content toc-card" id="mobiletoc">
  <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E5%88%86%E7%B1%BB"><span class="toc-text">二分分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFLogistic-%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="toc-text">什么是Logistic 回归？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="toc-text">Logistic 回归的Python实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%9F"><span class="toc-text">如何减少循环的次数？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-text">什么是双层神经网络？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%9F"><span class="toc-text">如何将双层神经网络进行向量化？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-text">如何选择激活函数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">激活函数的导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%89"><span class="toc-text">双层神经网络的梯度下降法（反向传播的实现）</span></a></li></ol></li></ol>
</div></main>
          <aside class="left-column">
            
            <div class="card card-author">
              
<img src="/img/touxiang.jpg" class="author-img">

<p class="author-name">Dicemy</p>
<p class="author-description">软件工程在读大学生</p>
<div class="author-message">
  <a class="author-posts-count" href="/archives">
    <span>28</span>
    <span>文章</span>
  </a>
  <a class="author-categories-count" href="/categories">
    <span>4</span>
    <span>分类</span>
  </a>
  <a class="author-tags-count" href="/tags">
    <span>16</span>
    <span>标签</span>
  </a>
</div>

<div class="author-card-society">
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener" href="https://space.bilibili.com/16023197">
        <i class="iconfont icon-bilibili society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a href="tencent://message/?uin=1583925267">
        <i class="iconfont icon-wechat society-icon"></i>
      </a>
    </div>
  
    <div class="author-card-society-icon">
      <a target="_blank" rel="noopener" href="https://github.com/Dicemy">
        <i class="iconfont icon-github society-icon"></i>
      </a>
    </div>
  
</div>

            </div>
            
            <div class="sticky-tablet">
  
  
  <article class="display-when-two-columns spacer">
    <div class="card card-content toc-card">
      <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E5%88%86%E7%B1%BB"><span class="toc-text">二分分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFLogistic-%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="toc-text">什么是Logistic 回归？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="toc-text">Logistic 回归的Python实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%9F"><span class="toc-text">如何减少循环的次数？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-text">什么是双层神经网络？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%9F"><span class="toc-text">如何将双层神经网络进行向量化？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-text">如何选择激活函数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">激活函数的导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%89"><span class="toc-text">双层神经网络的梯度下降法（反向传播的实现）</span></a></li></ol></li></ol>
    </div>
  </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right: 2px;"></i>分类</div>
  <div class="categories-list">
    
      <a href="/categories/竞赛">
        <div class="categories-list-item">
          竞赛
          <span class="categories-list-item-badge">16</span>
        </div>
      </a>
    
      <a href="/categories/技术">
        <div class="categories-list-item">
          技术
          <span class="categories-list-item-badge">7</span>
        </div>
      </a>
    
      <a href="/categories/文化课">
        <div class="categories-list-item">
          文化课
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/生活">
        <div class="categories-list-item">
          生活
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right: 2px;"></i>热门标签</div>
  <div class="tags-list">
    
    <a href="\tags\赛后总结" title="赛后总结"><div class="tags-list-item">赛后总结</div></a>
    
    <a href="\tags\搜索" title="搜索"><div class="tags-list-item">搜索</div></a>
    
    <a href="\tags\学习笔记" title="学习笔记"><div class="tags-list-item">学习笔记</div></a>
    
    <a href="\tags\组合数学" title="组合数学"><div class="tags-list-item">组合数学</div></a>
    
    <a href="\tags\DP" title="DP"><div class="tags-list-item">DP</div></a>
    
    <a href="\tags\数据库" title="数据库"><div class="tags-list-item">数据库</div></a>
    
    <a href="\tags\方法总结" title="方法总结"><div class="tags-list-item">方法总结</div></a>
    
    <a href="\tags\算法笔记" title="算法笔记"><div class="tags-list-item">算法笔记</div></a>
    
    <a href="\tags\博弈论" title="博弈论"><div class="tags-list-item">博弈论</div></a>
    
    <a href="\tags\文档阅读" title="文档阅读"><div class="tags-list-item">文档阅读</div></a>
    
    <a href="\tags\Linux" title="Linux"><div class="tags-list-item">Linux</div></a>
    
    <a href="\tags\最小生成树" title="最小生成树"><div class="tags-list-item">最小生成树</div></a>
    
    <a href="\tags\思考题" title="思考题"><div class="tags-list-item">思考题</div></a>
    
    <a href="\tags\乘法逆元" title="乘法逆元"><div class="tags-list-item">乘法逆元</div></a>
    
    <a href="\tags\双端队列" title="双端队列"><div class="tags-list-item">双端队列</div></a>
    
    <a href="\tags\数论" title="数论"><div class="tags-list-item">数论</div></a>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
          <aside class="right-column">
            <div class="sticky-widescreen">
  
  
  <article class="card card-content toc-card">
    <div class="toc-header"><i class="iconfont icon-menu" style="padding-right: 2px;"></i>目录</div>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E5%88%86%E7%B1%BB"><span class="toc-text">二分分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFLogistic-%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="toc-text">什么是Logistic 回归？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-%E5%9B%9E%E5%BD%92%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="toc-text">Logistic 回归的Python实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%9F"><span class="toc-text">如何减少循环的次数？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-text">什么是双层神经网络？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%9F"><span class="toc-text">如何将双层神经网络进行向量化？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-text">如何选择激活函数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">激活函数的导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%89"><span class="toc-text">双层神经网络的梯度下降法（反向传播的实现）</span></a></li></ol></li></ol>
  </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right: 2px;"></i>最近文章</div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-31</div>
        <a href="/25262.html"><div class="recent-posts-item-content">深度学习初探</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-10</div>
        <a href="/43344.html"><div class="recent-posts-item-content">河南大学暑假集训的日常（4）</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-10</div>
        <a href="/56657.html"><div class="recent-posts-item-content">河南大学暑假集训的日常（3）</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2021-08-10</div>
        <a href="/12868.html"><div class="recent-posts-item-content">Spring Boot入门</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
          </aside>
        </div>
      </div>
    </div>
  </div>
  
  <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>Copyright ©
          
          2020 -
          
          2021
        </span>
        &nbsp;
        <a href="/" class="footer-link">Dicemy 的博客 </a>
      </div>
    </div>

    
    
    
    
      <div class="footer-dsc">
        
        本站总访问量<span id="busuanzi_value_site_pv"></span>次
        
        
        <span>&nbsp;|&nbsp;</span>
        
        
        本站总访客数<span id="busuanzi_value_site_uv"></span>次
        
      </div>
      
    
</footer>
  <a role="button" id="scrollbutton" class="basebutton"  aria-label="回到顶部">
  <i class="iconfont icon-arrowleft button-icon"></i>
</a>
<a role="button" id="menubutton" class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a role="button" id="popbutton" class="basebutton" aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a role="button" id="darkbutton" class="basebutton darkwidget" aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a role="button" id="searchbutton" class="basebutton searchwidget" aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a>

  
  
  
  <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img');
    var i;
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a');
      wrapper.setAttribute('href', img[i].getAttribute('data-src'));
      wrapper.setAttribute('aria-label', 'illustration');
      wrapper.style.cssText = 'width: 100%; display: flex; justify-content: center;';
      if (img[i].alt) wrapper.dataset.caption = img[i].alt;
      wrapper.dataset.nolink = true;
      img[i].before(wrapper);
      wrapper.append(img[i]);
      var divWrap = document.createElement('div');
      divWrap.classList.add('gallery');
      wrapper.before(divWrap);
      divWrap.append(wrapper);
    }
    baguetteBox.run('.gallery');
  }
</script>
<script>loadScript("/js/lib/lightbox/baguetteBox.min.js", addImgLayout)</script>
  
  
  
<script src="/js/main.js"></script>

  
  <script>loadScript("/js/lib/busuanzi.min.js")</script>
  
  
  <script>
    var addLazyload = function () {
      var observer = lozad('.lozad', {
        load: function (el) {
          el.srcset = el.getAttribute('data-src');
        },
        loaded: function (el) {
          el.classList.add('loaded');
        }
      });
      observer.observe();
    }
  </script>
  <script>loadScript("/js/lib/lozad.min.js", addLazyload)</script>
  
  
</body>

</html>